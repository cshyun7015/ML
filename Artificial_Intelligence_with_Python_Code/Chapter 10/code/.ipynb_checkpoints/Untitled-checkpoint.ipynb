{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n",
      "\n",
      "Word tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n",
      "\n",
      "Word punct tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, \\\n",
    "        word_tokenize, WordPunctTokenizer\n",
    "\n",
    "# Define input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\" \n",
    "\n",
    "# Sentence tokenizer \n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize(input_text))\n",
    "\n",
    "# Word tokenizer\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))\n",
    "\n",
    "# WordPunct tokenizer\n",
    "print(\"\\nWord punct tokenizer:\")\n",
    "print(WordPunctTokenizer().tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "         writing           write            writ           write\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'randomize', \n",
    "        'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create various stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names), \n",
    "        '\\n', '='*68)\n",
    "\n",
    "# Stem each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), \n",
    "            lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n",
      "                 writing                 writing                   write\n",
      "                  calves                    calf                   calve\n",
      "                      be                      be                      be\n",
      "                 branded                 branded                   brand\n",
      "                   horse                   horse                   horse\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "               provision               provision               provision\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                    code                    code                    code\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'randomize', \n",
    "        'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create lemmatizer object \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of lemmatizer names for display\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), \n",
    "        '\\n', '='*75)\n",
    "\n",
    "# Lemmatize each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'),\n",
    "           lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 18 \n",
      "\n",
      "Chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
      "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
      "Chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
      "Chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
      "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
      "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
      "Chunk 7 ==> College . He has served as a border patrolman and \n",
      "Chunk 8 ==> of his staff were doing on the address involved co\n",
      "Chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
      "Chunk 10 ==> nursing homes In the area of `` community health s\n",
      "Chunk 11 ==> of its Angola policy prove harsh , there has been \n",
      "Chunk 12 ==> system which will prevent Laos from being used as \n",
      "Chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
      "Chunk 14 ==> . He is not interested in being named a full-time \n",
      "Chunk 15 ==> said , `` to obtain the views of the general publi\n",
      "Chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
      "Chunk 17 ==> making enforcement of minor offenses more effectiv\n",
      "Chunk 18 ==> to tell the people where he stands on the tax issu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Split the input text into chunks, where\n",
    "# each chunk contains N words\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "\n",
    "    output.append(' '.join(cur_chunk))\n",
    "\n",
    "    return output \n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Read the first 12000 words from the Brown corpus\n",
    "    input_data = ' '.join(brown.words()[:12000])\n",
    "\n",
    "    # Define the number of words in each chunk \n",
    "    chunk_size = 700\n",
    "\n",
    "    chunks = chunker(input_data, chunk_size)\n",
    "    print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print('Chunk', i+1, '==>', chunk[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      " ['and' 'are' 'be' 'by' 'county' 'for' 'in' 'is' 'it' 'of' 'on' 'one'\n",
      " 'said' 'state' 'that' 'the' 'to' 'two' 'was' 'which' 'with']\n",
      "\n",
      "Document term matrix:\n",
      "\n",
      "         Word     Chunk-1     Chunk-2     Chunk-3     Chunk-4     Chunk-5     Chunk-6     Chunk-7 \n",
      "\n",
      "         and          23           9           9          11           9          17          10\n",
      "         are           2           2           1           1           2           2           1\n",
      "          be           6           8           7           7           6           2           1\n",
      "          by           3           4           4           5          14           3           6\n",
      "      county           6           2           7           3           1           2           2\n",
      "         for           7          13           4          10           7           6           4\n",
      "          in          15          11          15          11          13          14          17\n",
      "          is           2           7           3           4           5           5           2\n",
      "          it           8           6           8           9           3           1           2\n",
      "          of          31          20          20          30          29          35          26\n",
      "          on           4           3           5          10           6           5           2\n",
      "         one           1           3           1           2           2           1           1\n",
      "        said          12           5           7           7           4           3           7\n",
      "       state           3           7           2           6           3           4           1\n",
      "        that          13           8           9           2           7           1           7\n",
      "         the          71          51          43          51          43          52          49\n",
      "          to          11          26          20          26          21          15          11\n",
      "         two           2           1           1           1           1           2           2\n",
      "         was           5           6           7           7           4           7           3\n",
      "       which           7           4           5           4           3           1           1\n",
      "        with           2           2           3           1           2           2           3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import brown\n",
    "from text_chunker import chunker \n",
    "\n",
    "# Read the data from the Brown corpus\n",
    "input_data = ' '.join(brown.words()[:5400])\n",
    "\n",
    "# Number of words in each chunk \n",
    "chunk_size = 800\n",
    "\n",
    "text_chunks = chunker(input_data, chunk_size)\n",
    "\n",
    "# Convert to dict items\n",
    "chunks = []\n",
    "for count, chunk in enumerate(text_chunks):\n",
    "    d = {'index': count, 'text': chunk}\n",
    "    chunks.append(d)\n",
    "\n",
    "# Extract the document term matrix\n",
    "count_vectorizer = CountVectorizer(min_df=7, max_df=20)\n",
    "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "\n",
    "# Extract the vocabulary and display it\n",
    "vocabulary = np.array(count_vectorizer.get_feature_names())\n",
    "print(\"\\nVocabulary:\\n\", vocabulary)\n",
    "\n",
    "# Generate names for chunks\n",
    "chunk_names = []\n",
    "for i in range(len(text_chunks)):\n",
    "    chunk_names.append('Chunk-' + str(i+1))\n",
    "\n",
    "# Print the document term matrix\n",
    "print(\"\\nDocument term matrix:\")\n",
    "formatted_text = '{:>12}' * (len(chunk_names) + 1)\n",
    "print('\\n', formatted_text.format('Word', *chunk_names), '\\n')\n",
    "for word, item in zip(vocabulary, document_term_matrix.T):\n",
    "    # 'item' is a 'csr_matrix' data structure\n",
    "    output = [word] + [str(freq) for freq in item.data]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training data: (2844, 40321)\n",
      "\n",
      "Input: You need to be careful with cars when you are driving on slippery roads \n",
      "Predicted category: Autos\n",
      "\n",
      "Input: A lot of devices can be operated wirelessly \n",
      "Predicted category: Electronics\n",
      "\n",
      "Input: Players need to be careful when they are close to goal posts \n",
      "Predicted category: Hockey\n",
      "\n",
      "Input: Political debates help us understand the perspectives of both sides \n",
      "Predicted category: Politics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the category map\n",
    "category_map = {'talk.politics.misc': 'Politics', 'rec.autos': 'Autos', \n",
    "        'rec.sport.hockey': 'Hockey', 'sci.electronics': 'Electronics', \n",
    "        'sci.med': 'Medicine'}\n",
    "\n",
    "# Get the training dataset\n",
    "training_data = fetch_20newsgroups(subset='train', \n",
    "        categories=category_map.keys(), shuffle=True, random_state=5)\n",
    "\n",
    "# Build a count vectorizer and extract term counts \n",
    "count_vectorizer = CountVectorizer()\n",
    "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
    "print(\"\\nDimensions of training data:\", train_tc.shape)\n",
    "\n",
    "# Create the tf-idf transformer\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)\n",
    "\n",
    "# Define test data \n",
    "input_data = [\n",
    "    'You need to be careful with cars when you are driving on slippery roads', \n",
    "    'A lot of devices can be operated wirelessly',\n",
    "    'Players need to be careful when they are close to goal posts',\n",
    "    'Political debates help us understand the perspectives of both sides'\n",
    "]\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_data.target)\n",
    "\n",
    "# Transform input data using count vectorizer\n",
    "input_tc = count_vectorizer.transform(input_data)\n",
    "\n",
    "# Transform vectorized data using tfidf transformer\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "\n",
    "# Predict the output categories\n",
    "predictions = classifier.predict(input_tfidf)\n",
    "\n",
    "# Print the outputs\n",
    "for sent, category in zip(input_data, predictions):\n",
    "    print('\\nInput:', sent, '\\nPredicted category:', \\\n",
    "            category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of end letters: 1\n",
      "Accuracy = 74.7%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> male\n",
      "\n",
      "Number of end letters: 2\n",
      "Accuracy = 78.79%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 3\n",
      "Accuracy = 77.22%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 4\n",
      "Accuracy = 69.98%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "\n",
      "Number of end letters: 5\n",
      "Accuracy = 64.63%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "from nltk.corpus import names\n",
    "\n",
    "# Extract last N letters from the input word\n",
    "# and that will act as our \"feature\"\n",
    "def extract_features(word, N=2):\n",
    "    last_n_letters = word[-N:]\n",
    "    return {'feature': last_n_letters.lower()}\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Create training data using labeled names available in NLTK\n",
    "    male_list = [(name, 'male') for name in names.words('male.txt')]\n",
    "    female_list = [(name, 'female') for name in names.words('female.txt')]\n",
    "    data = (male_list + female_list)\n",
    "\n",
    "    # Seed the random number generator\n",
    "    random.seed(5)\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Create test data\n",
    "    input_names = ['Alexander', 'Danielle', 'David', 'Cheryl']\n",
    "\n",
    "    # Define the number of samples used for train and test\n",
    "    num_train = int(0.8 * len(data))\n",
    "\n",
    "    # Iterate through different lengths to compare the accuracy\n",
    "    for i in range(1, 6):\n",
    "        print('\\nNumber of end letters:', i)\n",
    "        features = [(extract_features(n, i), gender) for (n, gender) in data]\n",
    "        train_data, test_data = features[:num_train], features[num_train:]\n",
    "        classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "        # Compute the accuracy of the classifier \n",
    "        accuracy = round(100 * nltk_accuracy(classifier, test_data), 2)\n",
    "        print('Accuracy = ' + str(accuracy) + '%')\n",
    "\n",
    "        # Predict outputs for input names using the trained classifier model\n",
    "        for name in input_names:\n",
    "            print(name, '==>', classifier.classify(extract_features(name, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training datapoints: 1600\n",
      "Number of test datapoints: 400\n",
      "\n",
      "Accuracy of the classifier: 0.735\n",
      "\n",
      "Top 15 most informative words:\n",
      "1. outstanding\n",
      "2. insulting\n",
      "3. vulnerable\n",
      "4. ludicrous\n",
      "5. uninvolving\n",
      "6. astounding\n",
      "7. avoids\n",
      "8. fascination\n",
      "9. darker\n",
      "10. anna\n",
      "11. seagal\n",
      "12. symbol\n",
      "13. affecting\n",
      "14. animators\n",
      "15. idiotic\n",
      "\n",
      "Movie review predictions:\n",
      "\n",
      "Review: The costumes in this movie were great\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.59\n",
      "\n",
      "Review: I think the story was terrible and the characters were very weak\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.8\n",
      "\n",
      "Review: People say that the director of the movie is amazing\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.6\n",
      "\n",
      "Review: This is such an idiotic movie. I will not recommend it to anyone.\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.87\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    " \n",
    "# Extract features from the input list of words\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "if __name__=='__main__':\n",
    "    # Load the reviews from the corpus \n",
    "    fileids_pos = movie_reviews.fileids('pos')\n",
    "    fileids_neg = movie_reviews.fileids('neg')\n",
    "     \n",
    "    # Extract the features from the reviews\n",
    "    features_pos = [(extract_features(movie_reviews.words(\n",
    "            fileids=[f])), 'Positive') for f in fileids_pos]\n",
    "    features_neg = [(extract_features(movie_reviews.words(\n",
    "            fileids=[f])), 'Negative') for f in fileids_neg]\n",
    "     \n",
    "    # Define the train and test split (80% and 20%)\n",
    "    threshold = 0.8\n",
    "    num_pos = int(threshold * len(features_pos))\n",
    "    num_neg = int(threshold * len(features_neg))\n",
    "     \n",
    "     # Create training and training datasets\n",
    "    features_train = features_pos[:num_pos] + features_neg[:num_neg]\n",
    "    features_test = features_pos[num_pos:] + features_neg[num_neg:]  \n",
    "\n",
    "    # Print the number of datapoints used\n",
    "    print('\\nNumber of training datapoints:', len(features_train))\n",
    "    print('Number of test datapoints:', len(features_test))\n",
    "     \n",
    "    # Train a Naive Bayes classifier \n",
    "    classifier = NaiveBayesClassifier.train(features_train)\n",
    "    print('\\nAccuracy of the classifier:', nltk_accuracy(\n",
    "            classifier, features_test))\n",
    "\n",
    "    N = 15\n",
    "    print('\\nTop ' + str(N) + ' most informative words:')\n",
    "    for i, item in enumerate(classifier.most_informative_features()):\n",
    "        print(str(i+1) + '. ' + item[0])\n",
    "        if i == N - 1:\n",
    "            break\n",
    "\n",
    "    # Test input movie reviews\n",
    "    input_reviews = [\n",
    "        'The costumes in this movie were great', \n",
    "        'I think the story was terrible and the characters were very weak',\n",
    "        'People say that the director of the movie is amazing', \n",
    "        'This is such an idiotic movie. I will not recommend it to anyone.' \n",
    "    ]\n",
    "\n",
    "    print(\"\\nMovie review predictions:\")\n",
    "    for review in input_reviews:\n",
    "        print(\"\\nReview:\", review)\n",
    "\n",
    "        # Compute the probabilities\n",
    "        probabilities = classifier.prob_classify(extract_features(review.split()))\n",
    "\n",
    "        # Pick the maximum value\n",
    "        predicted_sentiment = probabilities.max()\n",
    "\n",
    "        # Print outputs\n",
    "        print(\"Predicted sentiment:\", predicted_sentiment)\n",
    "        print(\"Probability:\", round(probabilities.prob(predicted_sentiment), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 contributing words to each topic:\n",
      "\n",
      "Topic 0\n",
      "\"empir\" ==> 4.1%\n",
      "\"time\" ==> 2.9%\n",
      "\"histor\" ==> 2.9%\n",
      "\"peopl\" ==> 2.9%\n",
      "\"expand\" ==> 2.9%\n",
      "\n",
      "Topic 1\n",
      "\"mathemat\" ==> 4.1%\n",
      "\"structur\" ==> 2.9%\n",
      "\"set\" ==> 2.9%\n",
      "\"call\" ==> 2.9%\n",
      "\"formul\" ==> 1.8%\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "\n",
    "# Load input data\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line[:-1])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Processor function for tokenizing, removing stop \n",
    "# words, and stemming\n",
    "def process(input_text):\n",
    "    # Create a regular expression tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Create a Snowball stemmer \n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # Get the list of stop words \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Tokenize the input string\n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "\n",
    "    # Remove the stop words \n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    \n",
    "    # Perform stemming on the tokenized words \n",
    "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "\n",
    "    return tokens_stemmed\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    # Load input data\n",
    "    data = load_data('data.txt')\n",
    "\n",
    "    # Create a list for sentence tokens\n",
    "    tokens = [process(x) for x in data]\n",
    "\n",
    "    # Create a dictionary based on the sentence tokens \n",
    "    dict_tokens = corpora.Dictionary(tokens)\n",
    "        \n",
    "    # Create a document-term matrix\n",
    "    doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]\n",
    "\n",
    "    # Define the number of topics for the LDA model\n",
    "    num_topics = 2\n",
    "\n",
    "    # Generate the LDA model \n",
    "    ldamodel = models.ldamodel.LdaModel(doc_term_mat, \n",
    "            num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
    "\n",
    "    num_words = 5\n",
    "    print('\\nTop ' + str(num_words) + ' contributing words to each topic:')\n",
    "    for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
    "        print('\\nTopic', item[0])\n",
    "\n",
    "        # Print the contributing words along with their relative contributions \n",
    "        list_of_strings = item[1].split(' + ')\n",
    "        for text in list_of_strings:\n",
    "            weight = text.split('*')[0]\n",
    "            word = text.split('*')[1]\n",
    "            print(word, '==>', str(round(float(weight) * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
